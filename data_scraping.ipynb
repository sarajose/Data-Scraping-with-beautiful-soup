{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d29216e",
   "metadata": {},
   "source": [
    "# Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "302cbc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375115e7",
   "metadata": {},
   "source": [
    "First, I access a csv file with a few URLs from some finnish companies which I have previously created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0602f3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          url\n",
      "0           https://wolt.com/\n",
      "1      https://www.nokia.com/\n",
      "2  https://metacoregames.com/\n"
     ]
    }
   ],
   "source": [
    "#Get data from a csv into a dataframe\n",
    "\n",
    "df = pd.read_csv('urls_db.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3999a452",
   "metadata": {},
   "source": [
    "Secondly, I define the function to get the http requests to a specific page. If the requests are completed succesfully, it creates an object with beautiful soup that retrieves the page content. If it is unsuccesfully, returns the response status code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b0d5bef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_request(url):\n",
    "    page = requests.get(url)\n",
    "    if page.status_code <= 299 and page.status_code >= 200:\n",
    "        print (\"The HTTP request has been successfully completed\")\n",
    "    \n",
    "        soup = bs(page.content, 'html5lib')\n",
    "        \n",
    "    else:\n",
    "        print(\"The HTTP request has not been successfully completed. \\nStatus code: \" + str(page.status_code))\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ca175e",
   "metadata": {},
   "source": [
    "Then, I created a function that gets the title, page url and description of the website. If no title is found, the text \"No title found\" will be saved instead. The same will happen for the rest to the variables. The function returns a tuple with the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e3fb9c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(soup):\n",
    "    title = soup.find(\"meta\", property=\"og:title\")\n",
    "    url = soup.find(\"meta\", property=\"og:url\")\n",
    "    description = soup.find(\"meta\", property=\"og:description\")\n",
    "\n",
    "    title = title[\"content\"] if title else \"No title found\"\n",
    "    url = url[\"content\"] if url else \"No url found\"\n",
    "    description = description[\"content\"] if url else \"No description found\"\n",
    "    \n",
    "    return title, url, description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da8e26d",
   "metadata": {},
   "source": [
    "A function to get the address is created. Because this information cannot be found on the main website page, the function looks and retrieves the contact page. From there, searches for Finland and retrieves the address if found. \n",
    "\n",
    "In case that the companies searched were based outside Finland, this function would need to be changed for another one which looked for a address based on structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b89e73b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_address(soup, url):\n",
    "    contact = soup.find(text=re.compile('Contact$'))\n",
    "    if contact: \n",
    "        parent = contact.parent\n",
    "        href = parent['href']\n",
    "\n",
    "        href = href.split(\"/\")\n",
    "        href = \"/\" + href[-1]\n",
    "\n",
    "        subdomain_url = url + href\n",
    "        subdomain = requests.get(subdomain_url)\n",
    "        soup2 = bs(subdomain.content, 'html5lib')\n",
    "\n",
    "        finland = soup2.findAll(text=re.compile('Finland$'))\n",
    "        parent = finland[0].parent\n",
    "        address = parent.text\n",
    "\n",
    "        return address\n",
    "    \n",
    "    else:\n",
    "        return \"No address found\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb389f2",
   "metadata": {},
   "source": [
    "Afterwards, it iterates throughout the urls in the dataframe and gets the title, url, description and address for each one of them and saves them in lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4dd18ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wolt.com/\n",
      "The HTTP request has been successfully completed\n",
      "Title: Wolt Delivery: Food and more – Finland\n",
      "Description: Wolt delivers from the best restaurants and stores around you.\n",
      "Address: WoltArkadiankatu 600100 HelsinkiSuomi, Finland\n",
      "\n",
      "https://www.nokia.com/\n",
      "The HTTP request has been successfully completed\n",
      "Title: Home | Nokia\n",
      "Description: Matkapuhelin-, kiinteiden ja pilvipalveluverkkojen teknologiajohtajana olemme mukana rakentamassa tuottavampaa, vastuullisempaa ja monimuotoisempaa maailmaa.\n",
      "Address: Visiting address:\n",
      "Karakaari 7\n",
      "02610 Espoo, Finland\n",
      "\n",
      "https://metacoregames.com/\n",
      "The HTTP request has been successfully completed\n",
      "Title: Metacore | The game company where players are the closest thing to a boss\n",
      "Description: We are the game company where players are the closest thing to a boss. That means our dream is to make our players’ dreams come true.\n",
      "Address: Porkkalankatu 24,00180 HelsinkiFinland\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titles = []\n",
    "urls = []\n",
    "descriptions = []\n",
    "addresses = []\n",
    "\n",
    "for row in df.index:\n",
    "    print(df['url'][row])\n",
    "    #print(type(df['url'][row]))\n",
    "    soup = get_request(df['url'][row])\n",
    "    title, url, description = get_metadata(soup)\n",
    "    address = get_address(soup, df['url'][row])\n",
    "    \n",
    "    titles.append(title)\n",
    "    urls.append(url)\n",
    "    descriptions.append(description)\n",
    "    addresses.append(address)\n",
    "    \n",
    "    print(\"Title: \" + title)\n",
    "    print(\"Description: \" + description)\n",
    "    print(\"Address: \" + address + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e691f3b",
   "metadata": {},
   "source": [
    "Finally, it populates de dataframe with lists obtained and exports and saves the dataframe into a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2d66354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title'] = titles\n",
    "df['Extracted Url'] = urls\n",
    "df['Description'] = descriptions\n",
    "df['Address'] = addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a50805ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"company_information2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "catapultEnv",
   "language": "python",
   "name": "catapultenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
